{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import s3fs\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask_ml\n",
    "import rasterio\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "import datetime\n",
    "import tempfile\n",
    "import boto3\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from dask.distributed import Client, progress\n",
    "import warnings\n",
    "\n",
    "gv.extension('matplotlib')\n",
    "\n",
    "gv.output(size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the most reliable way to install packages is to run `conda install -c pyviz geoviews-core dask-ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           AWS_NO_SIGN_REQUEST='YES',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000')\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_full_date_to_continous_day(year, month, day):\n",
    "    \"\"\"\n",
    "    Helper function if you wish to use month, day vs julian day\n",
    "    \"\"\"\n",
    "    return datetime.datetime(year, month, day).timetuple().tm_yday\n",
    "\n",
    "def get_geo_uri(year, day):\n",
    "    \"\"\"\n",
    "    returns list of geo uris\n",
    "    \"\"\"\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    files = []\n",
    "    \n",
    "    filepath = \"s3://noaa-goes17/ABI-L2-FDCC/%s/%s/*/*.nc\" % (str(year).zfill(4), str(day).zfill(3)) \n",
    "    files = fs.glob(filepath)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        raise Exception(\"No files found\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "def download_to_xarray(uri):\n",
    "    \"\"\"\n",
    "    Downloads file and directly loads it into xarray in memory\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        s3.download_file(Bucket=uri[:11], Key=uri[12:], Filename=temp_file.name)\n",
    "        datastore = xr.open_dataset(temp_file.name)\n",
    "        \n",
    "    return datastore\n",
    "\n",
    "def download_to_disk(uri):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    filename = uri[12:].replace(\"/\", \"-\")\n",
    "    if not os.path.exists(filename):\n",
    "        s3.download_file(Bucket=uri[:11], Key=uri[12:], Filename=filename)\n",
    "        \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many gaps in the NOAA GOES17 data and it's quite large, so we'll be using a subset of the data for the examples to show the general idea quickly. Dates can be changed if one wants the entirety of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUBBS = {\"year\": 2017, \"day1\":220, \"day2\":243}\n",
    "#CAMP = {\"year\": 2018, \"day1\":312, \"day2\":329}\n",
    "#WOOLSEY = {\"year\": 2018, \"day1\":312, \"day2\":325}\n",
    "\n",
    "CAMP = {\"year\": 2018, \"day1\":317, \"day2\":318}\n",
    "WOOLSEY = {\"year\": 2018, \"day1\":317, \"day2\":319}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a single goes file from s3 takes 1.7s so we first download to disk the portions of time we're interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp_uris = []\n",
    "woolsey_uris = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(CAMP[\"day2\"] - CAMP[\"day1\"]):\n",
    "    day = i + CAMP[\"day1\"]\n",
    "    camp_uris += get_geo_uri(CAMP[\"year\"], day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(WOOLSEY[\"day2\"] - WOOLSEY[\"day1\"]):\n",
    "    day = i + WOOLSEY[\"day1\"]\n",
    "    woolsey_uris += get_geo_uri(WOOLSEY[\"year\"], day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_filepaths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in camp_uris:\n",
    "    local_filepaths.append(download_to_disk(key))\n",
    "for key in woolsey_uris:\n",
    "    local_filepaths.append(download_to_disk(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_file_into_xarray(year, day, hour, localfilepaths):\n",
    "    \"\"\"\n",
    "    Returns an xarray of a single hour of data\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for element in localfilepaths:\n",
    "        split_file = element.split(\"-\")\n",
    "        if split_file[3] == str(year) and split_file[4] == str(day) and split_file[5] == str(hour).zfill(2):\n",
    "            files.append(element)\n",
    "    \n",
    "    if len(files) < 1:\n",
    "        raise Exception(\"File with that date is not found\")\n",
    "        \n",
    "    return xr.open_mfdataset(files,combine='nested',concat_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_xarray(data, vdims):\n",
    "    kdims = ['t', 'x', 'y']\n",
    "    xr_dataset = gv.Dataset(data, kdims=kdims, vdims=vdims)\n",
    "    image = xr_dataset.to(gv.Image, ['x', 'y'])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data quickly and easily accessible on disk, and any section of it can be visualized.\n",
    "To visualize each fire, you can simply stack the functions like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see part of the Camp Fire:\n",
    "# Warnings are ignored due to cartopy deprecation warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "visualize_xarray(load_local_file_into_xarray(2018, 318, 8, local_filepaths), ['Mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see multiple hours or days of the fire, you can concatenate them and feed it into the visualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see several consecutive hours:\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fires = []\n",
    "for i in range(5, 7):\n",
    "    fires.append(load_local_file_into_xarray(2018, 318, i, local_filepaths))\n",
    "\n",
    "visualize_xarray(xr.concat(fires, dim=\"time\"), ['Mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train an ML model on this data. I am going to first simply do binary classification on the entire image, whether or not the date is during the fire.\n",
    "\n",
    "If I were to do binary classification for every pixel individiually, it would at core look very similar. Using a model without locally correlated information, you can take the 2 dimensional image and convert it into a single vector, aligning it with the similarly transformed binary pixels.\n",
    "\n",
    "If the data does have locally correlated information, you want to use models that utilize that information, the canonical example being a convolutional neural net. In this case, you could use a U-Net with a binary loss function on each pixel, so that it directly creates the segmentation of each patch of images.\n",
    "\n",
    "The data I'm going to gather here is a toy example: it's much smaller than what will get actual results. The purpose is just to show the outline of how this would be done, and would be expanded and systemtized for a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fire_day1 = 200\n",
    "non_fire_day2 = 202\n",
    "non_fire_uris = []\n",
    "\n",
    "for i in range(non_fire_day1, non_fire_day2):\n",
    "    non_fire_uris += (get_geo_uri(2019, i))\n",
    "    \n",
    "for key in non_fire_uris:\n",
    "    local_filepaths.append(download_to_disk(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subset_of_data(year, day1, day2, y, data, labels):\n",
    "    for i in range(day1, day2):\n",
    "        for j in range(24):\n",
    "            try:\n",
    "                data.append(load_local_file_into_xarray(year, i, j, local_filepaths))\n",
    "                labels.append(y)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "load_subset_of_data(2019, non_fire_day1, non_fire_day1+1, 0, train_data, train_labels)\n",
    "load_subset_of_data(2019, non_fire_day1+1, non_fire_day1+2, 0, test_data, test_labels)\n",
    "load_subset_of_data(2018, 317, 318, 1, train_data, train_labels)\n",
    "load_subset_of_data(2018, 318, 319, 1, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a toy example, let's just use the mean fire temperature array in chunks of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = [], []\n",
    "testX, testY = [], []\n",
    "\n",
    "for data, label in zip(train_data, train_labels):\n",
    "    if (data.mean_fire_temperature.values.shape[0] % 6) == 0:\n",
    "        for i in range(0, data.mean_fire_temperature.values.shape[0], 6):\n",
    "            trainX.append(data.mean_fire_temperature.values[i:i+6])\n",
    "            trainY.append(label)\n",
    "    \n",
    "for data, label in zip(test_data, test_labels):\n",
    "    if (data.mean_fire_temperature.values.shape[0] % 6) == 0:\n",
    "        for i in range(0, data.mean_fire_temperature.values.shape[0], 6):\n",
    "            testX.append(data.mean_fire_temperature.values[i:i+6])\n",
    "            testY.append(label)\n",
    "        \n",
    "trainX = np.vstack(trainX)\n",
    "testX = np.vstack(testX)\n",
    "trainX = np.nan_to_num(trainX)\n",
    "testX = np.nan_to_num(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=2).fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our classifier, first let's make sure it fits the train set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = clf.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(trainY, train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = clf.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(testY, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we simply overfit the data, and didn't learn anything from it. This makes sense, as the amount of data is extremely small, and we didn't make sure that we had sufficient signal in the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask can be used to do some distribution of training, let's use it for hyperparameter grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='7GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\":[4,6], \"min_samples_split\": [2,3]}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=False,\n",
    "                           iid=True,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('dask'):\n",
    "    grid_search.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can observe the results of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "To get classification for each individual pixel, we'd need to acquire an array of boolean labels which matches the dimensions of the input for each location. This would look fundamentally to the above, but just feeding in 2d dimensional imaging data rather than agglomerated fire measurements.\n",
    "\n",
    "\n",
    "There are a number of ways to serve that imaging data. One could make a front end which hosts an open source element that displays the 2d classification matrix for each given date on a slider. There are a huge number of available libraries that will do this as a front end element that connects to a backend database hosting the matricies. This is not a very data heavy problem, as the users wouldn't request data extremely quickly. If they were frequently scrubbing, some caching will make it faster. \n",
    "\n",
    "If the primary use was not visual, but instead focused on serving a computational purpose, it would be very simple to provide the matrix as a REST response to a query with the date and area. To lower the amount of data that needs to get passed back and forth, it would make sense for the user to specify what type of data they wanted, as some of the data as demonstrated is quite small. Some common pitfalls here would be serving too much data without caching or specifying intelligently. For example, if the data changed only rarely, it would make sense to update the frontend with only deltas between pixels as that would lower data load. Depending on use, image compression could be used. I don't think this will end up being much of a problem, as modern browsers, computers and internet connections are plenty powerful enough to handle it. From the machine learning perspective, it's likely that one would want to process and generate the data, and populate the database automatically, rather than doing it on request, as the data isn't large enough that you'd run into problems storing it. \n",
    "\n",
    "### General thoughts\n",
    "\n",
    "This project had me do machine learning and data processing in a way I typically wouldn't. Usually, I avoid jupyter notebooks, as I find it difficult to do high performance processing with the way they track state and threads. If I were to do this assignment without the notebook and wanted to look at visualization, it may make sense to use a flask app as that way you can take advantage of webapp library support. The flask app would make python calls to a local server where the s3 files would be hosted. One could also simply use geoviews or matplotlib from the terminal, or you could use a python notebook but just for the visualization, all other code would occur in python scripts elsewhere. However, the nice thing about having the notebook format is that it allows someone else to walk through your code, something that isn't as easy with a large codebase. \n",
    "\n",
    "For model training and data loading, I would have done more parallel processing, taking advantage of multiple threads on the machine to gather data and do all the necessary preprocessing. There are of course a huge number of things that should be done in machine learning training that I didn't do here. Analyzing the data to ensure a good distribution, augmentation, normalization, etc. These are all things that should be done to ensure high performance on the task. It's also important that you have multiple unit tests for the model, with a set of typically out of distribution edge cases that can frequently trip up your model. In the deployment process, it's important to have these automatically run before deployment to ensure you are maintaining the quality that's necessary. Visualization is also important here: observing the output of your models can lead you to find patterns you would be unable to see otherwise, so having a good visuzliation engine is important, especially when trying to figure out how to improve performance in the early stages of model development. \n",
    "\n",
    "I think it's important to view code like the above as being a key product for your machine learning system, and not just the way to get to models running. When building your initial system, a robust, powerful development environment can enable fast prototyping for every other product down the line. It's extremely important that the code itself is modular and generalizable, not just for building new products, but also for building new code on top of it. Flexibility is key; a common junior engineer mistake is to hardcode too much, causing problems in the future when new needs arise. This is why I dislike jupyter notebooks for anything besides visualization: the cell format pushes people towards a bespoke coding schema for the purposes of that particular notebook. A way to avoid that is to create common codebases the notebooks import. \n",
    "\n",
    "One thing that's commonly overlooked is model check ins. It is important that weights and hyperparameters of models are stored for comparison during development. This is vital for making sure improvements are being made, as well as for observing hyperparamter impact on the qualitative aspects of model output. Metrics like accuracy or AUC can frequently obscure how the product is actually being used, and so the situation can occur where accuracy continues to rise, but customer satisfaction remains static or lowers. It's important that model weights and hyperparameters are saved in that situation, vs just the output metrics, so that one can go back and perform subjective analysis on their predictions.\n",
    "\n",
    "From the data perspective, doing your computation colacted with your data is absolutely key. This will enable much faster processing. This can be done with on-prem storage, or ensuring you're using cloud resources such that storage is colocated. This same notion also applies to disk vs memory. Frequently deep learning is handicapped by data going from disk->ram->vram constantly, rather than ensuring you're getting the most data you possibly can. Programs like dask can help with this, and it's important to chunk correctly. \n",
    "\n",
    "Usually, it's a good idea to develop with one environment, and then send off a model, hyperparameters and data properties to a server that is optimized just for training. A third environment exists for just inference, as speed is absolutely key if there's to be a great deal of data processed. I think it's important that these environments are split because the desires for each ofthem are so different. It's because of this line of thinking that makes me believe doing data loading, model development, visualization, and inference in one notebook is not going to be the optimal plan. \n",
    "\n",
    "There are a number of key steps I always stick to for model development, described by Andrej Karpathy [here](https://karpathy.github.io/2019/04/25/recipe/). These are mainly focused on the ML model development rather than the surrounding software infrastructure. However, if the infrastructure is built with these steps in mind, it can make performing them very easy and increase rate of prototyping.\n",
    "\n",
    "I found the NOAA GOES data to be quite interesting and potentially very valuable. It had an extremely wide range of signals captured, and it would be fun to explore how autocorrelated they all are, as well as correlations to signals captured from other sources. In addition, one might be able to use ML models trained on the GOES data to build simulations and discern general principles, which could allow for rough predictions on rare events where labeled data isn't available. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
